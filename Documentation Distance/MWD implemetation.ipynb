{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distance Implementation by cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# 读取Glove文件。 注意： 不要试图修改文件以及路径\n",
    "#glovefile = open(\"glove.6B.100d.txt\",\"r\",encoding=\"utf-8\")  \n",
    "\n",
    "\n",
    "# TODO: 编写WMD函数来计算两个句子之间的相似度\n",
    "def convert2freq(wdlist):\n",
    "    \"\"\"\n",
    "    input: a list of words.\n",
    "    return : a dictionary of unique words and the corresponding frequency.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for wd in wdlist:\n",
    "        if wd in d:\n",
    "            d[wd.lower()] += 1\n",
    "        else:\n",
    "            d[wd.lower()] = 1\n",
    "    \n",
    "    return d\n",
    "\n",
    "def sent_pre_1(sent, flag):\n",
    "    \"\"\"\n",
    "    input1 : sentence\n",
    "    input2 : True = remove the stop words; False = otherwise\n",
    "    output : a dictionary of word frequency\n",
    "    \"\"\"\n",
    "    # get the word list from sentense\n",
    "    if flag:\n",
    "        text_tokens = tokenizer.tokenize(sent)\n",
    "        wdlist = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    else:\n",
    "        wdlist = sent.strip().split()\n",
    "     \n",
    "    wddict = convert2freq(wdlist)\n",
    "    return wddict\n",
    "    \n",
    "def sent_pre_2(wdmatrix, wddict):\n",
    "    \"\"\"\n",
    "    input : word2vec matrix of wdlist\n",
    "    output : v, d. prepare the input for MWD.\n",
    "    \"\"\"\n",
    "    d, v = [], []\n",
    "    for item in wdmatrix:\n",
    "        d.append(wddict[item[0]])\n",
    "        v.append(item[0])\n",
    "    total = sum(d)\n",
    "    d = np.array(d)\n",
    "    d = d/total   \n",
    "    return d, v\n",
    "    \n",
    "        \n",
    "def WMD (sent1, sent2, flag):\n",
    "    \"\"\"\n",
    "    这是主要的函数模块。参数sent1是第一个句子， 参数sent2是第二个句子，可以认为没有经过分词。\n",
    "    在英文里，用空格作为分词符号。\n",
    "    \n",
    "    在实现WMD算法的时候，需要用到LP Solver用来解决Transportation proboem. 请使用http://cvxopt.org/examples/tutorial/lp.html\n",
    "    也可以参考blog： https://scaron.info/blog/linear-programming-in-python-with-cvxopt.html\n",
    "    \n",
    "    需要做的事情为：\n",
    "    \n",
    "    1. 对句子做分词： 调用 .split() 函数即可\n",
    "    2. 获取每个单词的词向量。这需要读取文件之后构建embedding matrix. \n",
    "    3. 构建lp问题，并用solver解决\n",
    "    \n",
    "    可以自行定义其他的函数，但务必不要改写WMD函数名。测试时保证WMD函数能够正确运行。\n",
    "    \"\"\"\n",
    "    wddict1 = sent_pre_1(sent1, flag)\n",
    "    wddict2 = sent_pre_1(sent2, flag)\n",
    "    #print(wddict1)\n",
    "    #print(wddict2)\n",
    "    \n",
    "    # fetch the word2vec from glove database\n",
    "    wdmatrix1 = []\n",
    "    wdmatrix2 = []\n",
    "    \n",
    "    glovefile = open(\"glove.6B.100d.txt\",\"r\",encoding=\"utf-8\")  \n",
    "    for line in glovefile.readlines():\n",
    "        current = line.split()\n",
    "        #print(current[0])\n",
    "        if current[0] in wddict1:\n",
    "            wdmatrix1.append(current)\n",
    "        if current[0] in wddict2:\n",
    "            wdmatrix2.append(current)    \n",
    "    glovefile.close()\n",
    "    \n",
    "    # calculate d1 and d2\n",
    "    \n",
    "    d1, v1 = sent_pre_2(wdmatrix1, wddict1)\n",
    "    d2, v2 = sent_pre_2(wdmatrix2, wddict2)\n",
    "    #print(d1, d2)\n",
    "    #print(v1, v2)\n",
    "    \n",
    "    # calculate c(i, j)\n",
    "    n = len(wddict1)\n",
    "    m = len(wddict2)\n",
    "    \n",
    "    embedding = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            xi = np.array(wdmatrix1[i][1:])\n",
    "            xi = xi.astype(np.float)\n",
    "            xj = np.array(wdmatrix2[j][1:])\n",
    "            xj = xj.astype(np.float)\n",
    "            embedding[i][j] = np.linalg.norm(xi - xj)\n",
    "    #print(embedding)\n",
    "    \n",
    "    # optimization in calculation WMD\n",
    "    c = embedding.flatten().tolist()\n",
    "    #print(c)\n",
    "    \n",
    "    # construct A\n",
    "    # the contrain in i\n",
    "    Ai_total = []\n",
    "    for i in range(n):\n",
    "        Ai = np.zeros((n, m))\n",
    "        Ai[i,:] = 1\n",
    "        Ai_total.append(Ai.flatten())\n",
    "    Ai_total = np.array(Ai_total)\n",
    "    #print('Ai')\n",
    "    #print(Ai_total)\n",
    "    \n",
    "    # the contain in j\n",
    "    Aj_total = []\n",
    "    for j in range(m):\n",
    "        Aj = np.zeros((n, m))\n",
    "        Aj[:,j] = 1\n",
    "        Aj_total.append(Aj.flatten())\n",
    "    Aj_total = np.array(Aj_total)\n",
    "    #print('Aj')\n",
    "    #print(Aj_total)\n",
    "    \n",
    "    # positive entries\n",
    "    A_total = []\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            A = np.zeros((n, m))\n",
    "            A[i,j] = -1\n",
    "            A_total.append(A.flatten())\n",
    "            \n",
    "    A = np.concatenate((Ai_total, Aj_total, -Ai_total, -Aj_total, A_total))\n",
    "    A = (A.T).tolist()\n",
    "    #print(A)\n",
    "    \n",
    "    # construct b\n",
    "    b = np.concatenate((d1, d2, -d1, -d2, np.zeros(n*m))).tolist()\n",
    "    #print(b)\n",
    "    \n",
    "    # do optimization\n",
    "    from cvxopt import matrix, solvers\n",
    "    opA = matrix(A)\n",
    "    opb = matrix(b)\n",
    "    opc = matrix (c)\n",
    "    sol = solvers.lp(opc, opA, opb)\n",
    "    return sol['primal objective']\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are', 'have', 'it', 'its', 's', 'wouldn', 'weren', 'now', 'all', 'aren', 'hasn', 'same', 'your', 'were', 'further', 'has', 'no', 'o', 't', 'how', 'needn', \"isn't\", \"it's\", 'themselves', 'during', 'which', 'and', 'an', 'won', 'shouldn', \"she's\", 'himself', 'any', 'being', 'whom', 'in', 'he', 'his', 'below', 'own', 'she', 'some', 'by', 'both', 'then', 'down', 'against', 'about', 'on', 'after', 'doesn', \"aren't\", \"you're\", 'wasn', 'ma', 'myself', 'am', 'yourselves', \"couldn't\", 'what', 'because', \"don't\", \"shan't\", \"wasn't\", 'very', \"weren't\", \"doesn't\", 'shan', 'their', 'nor', 'above', 'those', \"mustn't\", 'for', 'why', 'or', 'few', \"shouldn't\", 'that', 're', 'had', 'than', 'couldn', \"hasn't\", 'where', 'up', 'before', 'they', 'him', 'did', 'other', 'do', 'when', \"haven't\", 'herself', 'been', 'if', 'just', \"won't\", \"mightn't\", 'should', 'most', 'yours', 'of', 'through', 'under', \"wouldn't\", 'here', 'me', 'but', 'itself', \"that'll\", 'as', 'off', 'my', 'we', \"you'll\", 'to', 'from', 'd', 'over', 'didn', 'isn', 'there', 'is', 'her', 'the', 'more', 'so', 'was', 'ours', \"you've\", 'a', 'can', 'ain', 'our', 'mightn', 'only', 'between', 'ourselves', 'them', 'who', 'don', 'with', 've', 'does', 'be', 'not', 'i', 'theirs', 'doing', \"needn't\", \"should've\", \"didn't\", 'while', 'will', 'y', 'these', 'having', 'such', 'this', 'm', \"hadn't\", 'out', 'yourself', \"you'd\", 'until', 'hadn', 'once', 'haven', 'you', 'hers', 'again', 'at', 'mustn', 'll', 'into', 'too', 'each'}\n"
     ]
    }
   ],
   "source": [
    "# print out the stop words\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 1, 'like': 1, 'car': 1}\n"
     ]
    }
   ],
   "source": [
    "print(sent_pre_1(\"people like this car.\", True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  5.2346e+00  5.2346e+00  6e+01  4e+00  4e-01  1e+00\n",
      " 1:  4.0716e+00  4.1724e+00  3e+00  3e-01  3e-02  2e-01\n",
      " 2:  4.7335e+00  4.7543e+00  3e-01  4e-02  5e-03  3e-02\n",
      " 3:  4.7456e+00  4.7460e+00  8e-03  1e-03  1e-04  6e-04\n",
      " 4:  4.7468e+00  4.7468e+00  8e-05  1e-05  1e-06  6e-06\n",
      " 5:  4.7468e+00  4.7468e+00  8e-07  1e-07  1e-08  6e-08\n",
      " 6:  4.7468e+00  4.7468e+00  8e-09  1e-09  1e-10  6e-10\n",
      "Optimal solution found.\n",
      "4.7468361312141\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  5.4718e+00  5.4718e+00  1e+02  7e+00  5e-01  1e+00\n",
      " 1:  3.9341e+00  4.1112e+00  6e+00  5e-01  4e-02  2e-01\n",
      " 2:  5.1091e+00  5.1651e+00  8e-01  1e-01  7e-03  7e-02\n",
      " 3:  5.1667e+00  5.1787e+00  2e-01  3e-02  2e-03  2e-02\n",
      " 4:  5.1805e+00  5.1807e+00  5e-03  7e-04  5e-05  3e-04\n",
      " 5:  5.1812e+00  5.1812e+00  5e-05  7e-06  5e-07  3e-06\n",
      " 6:  5.1812e+00  5.1812e+00  5e-07  7e-08  5e-09  3e-08\n",
      "Optimal solution found.\n",
      "5.181171843856026\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  5.3050e+00  5.3050e+00  4e+01  2e+00  5e-01  1e+00\n",
      " 1:  5.0663e+00  5.2832e+00  3e+00  3e-01  7e-02  4e-01\n",
      " 2:  5.9503e+00  5.9654e+00  7e-02  1e-02  2e-03  2e-02\n",
      " 3:  5.9679e+00  5.9681e+00  7e-04  1e-04  2e-05  2e-04\n",
      " 4:  5.9681e+00  5.9681e+00  7e-06  1e-06  2e-07  2e-06\n",
      " 5:  5.9681e+00  5.9681e+00  7e-08  1e-08  2e-09  2e-08\n",
      "Optimal solution found.\n",
      "5.968084463570138\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  4.8407e+00  4.8407e+00  2e+01  1e+00  4e-01  1e+00\n",
      " 1:  5.5374e+00  5.6334e+00  9e-01  1e-01  3e-02  2e-01\n",
      " 2:  6.0457e+00  6.0467e+00  9e-03  1e-03  3e-04  2e-03\n",
      " 3:  6.0508e+00  6.0508e+00  9e-05  1e-05  3e-06  2e-05\n",
      " 4:  6.0508e+00  6.0508e+00  9e-07  1e-07  3e-08  2e-07\n",
      " 5:  6.0508e+00  6.0508e+00  9e-09  1e-09  3e-10  2e-09\n",
      "Optimal solution found.\n",
      "6.050813699978407\n",
      "     pcost       dcost       gap    pres   dres   k/t\n",
      " 0:  5.0780e+00  5.0780e+00  9e+01  4e+00  7e-01  1e+00\n",
      " 1:  2.7834e+00  2.8815e+00  4e+00  2e-01  4e-02  2e-01\n",
      " 2:  3.3821e+00  3.3945e+00  2e-01  2e-02  3e-03  2e-02\n",
      " 3:  3.3926e+00  3.3927e+00  2e-03  2e-04  3e-05  2e-04\n",
      " 4:  3.3927e+00  3.3927e+00  2e-05  2e-06  3e-07  2e-06\n",
      " 5:  3.3927e+00  3.3927e+00  2e-07  2e-08  3e-09  2e-08\n",
      "Optimal solution found.\n",
      "3.3926717320683544\n"
     ]
    }
   ],
   "source": [
    "##       \n",
    "print (WMD(\"people like this car.\", \"those guys enjoy driving that.\", True))\n",
    "print (WMD(\"implementation is quite time consuming\", \"those guys enjoy driving that\", False))\n",
    "print (WMD(\"people\", \"those guys enjoy driving that\", True))\n",
    "print (WMD(\"people\", \"those guys\", True))\n",
    "print (WMD(\"president talk chicago\", \"president speech illinois\", True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
